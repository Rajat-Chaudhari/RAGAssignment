{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1706614740061
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:38:59.8972489Z",
       "execution_start_time": "2024-01-30T11:38:49.706611Z",
       "livy_statement_state": "available",
       "parent_msg_id": "9815b530-8e6b-410a-a2b2-34a3cb521d3c",
       "queued_time": "2024-01-30T11:31:23.760322Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 15
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 15, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-30T11:34:59.526392] post artifact meta request successfully.\n",
      "[2024-01-30T11:34:59.648331] post artifact meta request successfully.\n",
      "Successfully add logs to artifact services.\n",
      "Start to mount artifact container with mount point /AmlJobLogs/dcid.092a2b97-655e-470a-b64a-a070e2b0691b\n",
      "Successfully mount artifact container to path:  /synfs/notebook/AmlJobLogs/dcid.092a2b97-655e-470a-b64a-a070e2b0691b\n",
      "Starting log upload threads\n"
     ]
    },
    {
     "data": {},
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28.1\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from openai==0.28.1) (3.7.4.post0)\n",
      "Requirement already satisfied: requests>=2.20 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from openai==0.28.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from openai==0.28.1) (4.61.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (2021.5.30)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (3.10.0.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (5.1.0)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.28.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 KB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 KB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-community<0.1,>=0.0.14\n",
      "  Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from langchain) (2.25.1)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.16\n",
      "  Downloading langchain_core-0.1.17-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from langchain) (5.4.1)\n",
      "Collecting langsmith<0.1,>=0.0.83\n",
      "  Downloading langsmith-0.0.84-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from langchain) (1.4.20)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from langchain) (1.19.4)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.1.0)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting anyio<5,>=3\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 KB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<24.0,>=23.2\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting pydantic-core==2.16.1\n",
      "  Downloading pydantic_core-2.16.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests<3,>=2->langchain) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2021.5.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.0)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Installing collected packages: typing-extensions, tenacity, sniffio, packaging, jsonpointer, frozenlist, exceptiongroup, async-timeout, typing-inspect, pydantic-core, marshmallow, jsonpatch, anyio, annotated-types, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Not uninstalling typing-extensions at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 7.0.0\n",
      "    Not uninstalling tenacity at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'tenacity'. No files were found to uninstall.\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.0\n",
      "    Not uninstalling packaging at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'packaging'. No files were found to uninstall.\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 3.0.1\n",
      "    Not uninstalling async-timeout at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'async-timeout'. No files were found to uninstall.\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.7.4.post0\n",
      "    Not uninstalling aiohttp at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'aiohttp'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.2.0 async-timeout-4.0.3 dataclasses-json-0.6.3 exceptiongroup-1.2.0 frozenlist-1.4.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.84 marshmallow-3.20.2 packaging-23.2 pydantic-2.6.0 pydantic-core-2.16.1 sniffio-1.3.0 tenacity-8.2.3 typing-extensions-4.9.0 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.0/777.0 KB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.26.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 KB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, charset-normalizer, requests, tiktoken\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.7.6\n",
      "    Not uninstalling regex at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'regex'. No files were found to uninstall.\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Not uninstalling requests at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed charset-normalizer-3.3.2 regex-2023.12.25 requests-2.31.0 tiktoken-0.5.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting chromadb==0.3.29\n",
      "  Downloading chromadb-0.3.29-py3-none-any.whl (396 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 KB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting clickhouse-connect>=0.5.7\n",
      "  Downloading clickhouse_connect-0.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (986 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m986.1/986.1 KB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.21.6\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2.0,>=1.9\n",
      "  Downloading pydantic-1.10.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from chromadb==0.3.29) (4.9.0)\n",
      "Requirement already satisfied: requests>=2.28 in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from chromadb==0.3.29) (2.31.0)\n",
      "Collecting duckdb>=0.7.1\n",
      "  Downloading duckdb-0.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hnswlib>=0.7\n",
      "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas>=1.3\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting posthog>=2.4.0\n",
      "  Downloading posthog-3.3.3-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.16.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting graphlib-backport>=1.0.3\n",
      "  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n",
      "Collecting tqdm>=4.65.0\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi==0.85.1\n",
      "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette==0.20.4\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (4.2.0)\n",
      "Requirement already satisfied: pytz in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2021.1)\n",
      "Collecting zstandard\n",
      "  Downloading zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2021.5.30)\n",
      "Requirement already satisfied: urllib3>=1.26 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (1.26.4)\n",
      "Collecting lz4\n",
      "  Downloading lz4-4.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (3.15.8)\n",
      "Requirement already satisfied: flatbuffers in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (1.12)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 KB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.16.0)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from requests>=2.28->chromadb==0.3.29) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests>=2.28->chromadb==0.3.29) (2.10)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 KB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (8.0.1)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (5.4.1)\n",
      "Collecting python-dotenv>=0.13\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
      "  Downloading uvloop-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httptools>=0.5.0\n",
      "  Downloading httptools-0.6.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 KB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.4\n",
      "  Downloading websockets-12.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 KB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (3.8.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.2.0)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp38-cp38-linux_x86_64.whl size=1574513 sha256=6aac69c0ad95326cf51a1e376ea421b18e767decd9fe393e115fbcf7f02c057a\n",
      "  Stored in directory: /home/trusted-service-user/.cache/pip/wheels/c9/52/92/b4b22fa3e652073584067109b5094dbd5c73b42738e41213f6\n",
      "Successfully built hnswlib\n",
      "Installing collected packages: mpmath, monotonic, zstandard, websockets, uvloop, tzdata, tqdm, sympy, python-dotenv, python-dateutil, pydantic, pulsar-client, overrides, numpy, lz4, humanfriendly, httptools, h11, graphlib-backport, fsspec, duckdb, backoff, watchfiles, uvicorn, starlette, posthog, pandas, huggingface_hub, hnswlib, coloredlogs, clickhouse-connect, tokenizers, onnxruntime, fastapi, chromadb\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Not uninstalling tqdm at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'tqdm'. No files were found to uninstall.\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.0\n",
      "    Uninstalling pydantic-2.6.0:\n",
      "      Successfully uninstalled pydantic-2.6.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Not uninstalling numpy at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.6.1\n",
      "    Not uninstalling fsspec at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'fsspec'. No files were found to uninstall.\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.3\n",
      "    Not uninstalling pandas at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'pandas'. No files were found to uninstall.\n",
      "  Attempting uninstall: onnxruntime\n",
      "    Found existing installation: onnxruntime 1.7.2\n",
      "    Not uninstalling onnxruntime at /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages, outside environment /nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555\n",
      "    Can't uninstall 'onnxruntime'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.9.0 which is incompatible.\n",
      "pmdarima 1.8.2 requires numpy~=1.19.0, but you have numpy 1.24.4 which is incompatible.\n",
      "koalas 1.8.0 requires numpy<1.20.0,>=1.14, but you have numpy 1.24.4 which is incompatible.\n",
      "azureml-opendatasets 1.34.0 requires pandas<=2.0.0,>=0.21.0, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.29 clickhouse-connect-0.7.0 coloredlogs-15.0.1 duckdb-0.9.2 fastapi-0.85.1 fsspec-2023.12.2 graphlib-backport-1.0.3 h11-0.14.0 hnswlib-0.8.0 httptools-0.6.1 huggingface_hub-0.20.3 humanfriendly-10.0 lz4-4.3.3 monotonic-1.6 mpmath-1.3.0 numpy-1.24.4 onnxruntime-1.16.3 overrides-7.7.0 pandas-2.0.3 posthog-3.3.3 pulsar-client-3.4.0 pydantic-1.10.14 python-dateutil-2.8.2 python-dotenv-1.0.1 starlette-0.20.4 sympy-1.12 tokenizers-0.15.1 tqdm-4.66.1 tzdata-2023.4 uvicorn-0.27.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 zstandard-0.22.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {},
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf==3.12.1\n",
      "  Downloading pypdf-3.12.1-py3-none-any.whl (254 kB)\n",
      "\u001b[K     |████████████████████████████████| 254 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from pypdf==3.12.1) (3.10.0.0)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.12.1\n",
      "Warning: PySpark kernel has been restarted to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OpenAI\n",
    "%pip install openai==0.28.1\n",
    " \n",
    "# LangChain package\n",
    "%pip install -U langchain\n",
    " \n",
    "#This is needed in order to for OpenAIEmbeddings\n",
    "%pip install tiktoken\n",
    " \n",
    "# %pip install arxiv\n",
    " \n",
    "# %pip install langchain openai google-search-results\n",
    " \n",
    "# %pip install wikipedia\n",
    " \n",
    "# %pip install numexpr  #LLMMathChain\n",
    " \n",
    " \n",
    "#%pip install PyPDF2\n",
    " \n",
    "%pip install faiss-cpu\n",
    " \n",
    "%pip install chromadb==0.3.29\n",
    "\n",
    "!pip install pypdf==3.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1706614748262
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:39:08.2078781Z",
       "execution_start_time": "2024-01-30T11:39:05.7529973Z",
       "livy_statement_state": "available",
       "parent_msg_id": "0bcd65a1-a74f-40c3-b122-017b480f38d7",
       "queued_time": "2024-01-30T11:31:23.9948088Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 21
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 21, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-30T11:39:05.749070] get artifact sas uri successfully. origin is ExperimentRun artifact path is logs/azureml/driver/stdout\n",
      "Successfully add logs to artifact services.\n",
      "Start to mount artifact container with mount point /AmlJobLogs/dcid.092a2b97-655e-470a-b64a-a070e2b0691b\n",
      "Successfully mount artifact container to path:  /synfs/notebook/AmlJobLogs/dcid.092a2b97-655e-470a-b64a-a070e2b0691b\n",
      "Starting log upload threads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\" \n",
    "openai.api_base = \"****\"  # Your Azure OpenAI resource's endpoint value.\n",
    "openai.api_key = \"*****\"\n",
    "\n",
    "DEPLOYMENT_NAME = \"gpt-35-turbo-inter02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1706614788208
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:39:48.1759157Z",
       "execution_start_time": "2024-01-30T11:39:47.3173518Z",
       "livy_statement_state": "available",
       "parent_msg_id": "c23ffcf6-c6bb-48b7-9234-ad7ba976a450",
       "queued_time": "2024-01-30T11:39:47.1871192Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 23
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 23, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1706614798692
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:39:58.6056334Z",
       "execution_start_time": "2024-01-30T11:39:57.0717633Z",
       "livy_statement_state": "available",
       "parent_msg_id": "d93f4eab-2569-4acd-befa-4aa9243ceb57",
       "queued_time": "2024-01-30T11:39:56.9628364Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 24
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 24, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\" \\nAll Sciences  Proceedings  \\nhttp://as -proceeding.com/  5th International Conference on Applied \\nEngineering and Natural Sciences  \\n \\nJuly 10 -12, 2023 : Konya, Turkey  \\n \\nhttps://www.icaens.com/  © 20 23 Published by All Sciences Proceedings  \\n \\n1050  \\n \\n \\nCreating Large Language Model Applications Utilizing LangChain: A \\nPrimer on Developing LLM Apps Fast  \\nOguzhan Topsakal1*, and Tahi r Cetin Akinci  2  \\n1Computer Science Department , Florida Polytechnic University, FL, USA  \\n2WCGEC , University  of California at Riverside , CA, USA  \\n*(otopsakal @floridapoly .edu) Email of the corresponding author  \\n \\nAbstract – This study focuses on the utilization of Large Language Models (LLMs) for the rapid \\ndevelopment of applications, with a spotlight on L angChain, an open -source software library. LLMs have \\nbeen rapidly adopted due to their capabilities in a range of tasks, including essay composition, code \\nwriting, explanation, and debugging, with OpenAI’s ChatGPT popularizing their usage among millions of  \\nusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke \\nAI applications using LLMs. LangChain has been widely recognized in the AI community for its ability \\nto seamlessly interact with various data sources an d applications. The paper provides a n examination of \\nLangChain's core features, including its components and chains, acting as modular abstractions and \\ncustomizable, use -case-specific pipelines, respectively. Through a series of practical examples, the stu dy \\nelucidates the potential of this framework in fostering the swift development of LLM -based applications .   \\n \\nKeywords – Large Language Models, LangChain, Concepts, Application,  ChatGPT, NLP, GPT  \\n \\nI. INTRODUCTION  \\nThe past decade has witnessed an unparalleled \\nevolution in the realm of artificial intelligence (AI). \\nThis period, characterized by the ascendancy of \\ndeep learning through the utilization of neural \\nnetworks, has resulted in significant enhancements \\nin capabilities pertaining to image and speech \\nrecognition. One salien t milestone highlighting this \\nprogress is the ImageNet Large Scale Visual \\nRecognition Challenge, which effectively \\ndemonstrated the prowess of AI capabilities in \\nimage recognition . [1][2]  \\nAnother major milestone in the AI landscape is \\nthe successful implem entation of reinforcement \\nlearning, as exemplified by DeepMind's AlphaGo \\nand AlphaZero.  [3] These innovations have \\ndemonstrated extraordinary performance in \\ncomplex games, such as Go and Chess, using self -\\nplay algorithms, thereby signifying a leap forward \\nin reinforcement learning techniques. In parallel, \\nthe evolution of generative models has facilitated the creation of convincingly realistic synthetic \\nmultimedia content . \\nDuring the same period, the field of natural \\nlanguage processing (NLP) experienced \\nremarkable transformations. The advent of \\nadvanced models, exemplified by the likes of \\nBERT (Bidirectional Encoder Representations \\nfrom Transformers ) by Google [4] and GPT \\n(Generative Pretrained Transformer ) by OpenAI  \\n[5], and T5 (Text -to-Text Transfer Transformer) by \\nGoogle  [6] has fostered significant improvements \\nin machine translation, sentiment analysis, and text \\ngeneration, thus ushering in a new era for NLP.  \\nBERT, GPT,  T5 and similar technologies all \\nutilized transformers architecture and were trained \\non huge amount of data and hence named  as Large \\nLanguage Models (LLMs).  [7] As LLMs were \\ntrained using more and more data, and \\nencompassed more parameters, their capabilit ies \\nincreased. For example, GPT -1 (June 2018 ), GPT -\\n2 (February 2019 ), and GPT -3 (June 2020 ) had 117 \" metadata={'source': 'Users/rajat.chaudhari/CreatingLargeLanguageModelApplicationsUtilizingLangChain-APrimeronDevelopingLLMAppsFast (1).pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "loader = PyPDFLoader(\"Users/rajat.chaudhari/RAGInputPaper.pdf\")\n",
    "documents = loader.load()\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. Document Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1706614805521
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:40:05.4613068Z",
       "execution_start_time": "2024-01-30T11:40:05.1534634Z",
       "livy_statement_state": "available",
       "parent_msg_id": "e29cf781-46fe-4608-b27a-589625fb2802",
       "queued_time": "2024-01-30T11:40:05.0345065Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 25
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 25, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1706614813116
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:40:12.8984113Z",
       "execution_start_time": "2024-01-30T11:40:12.5810539Z",
       "livy_statement_state": "available",
       "parent_msg_id": "af8c3862-1fdd-4dd1-a60f-81ac6019fcd9",
       "queued_time": "2024-01-30T11:40:12.4650613Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 26
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 26, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings= OpenAIEmbeddings(\n",
    "        deployment=\"text-embedding-ada-002-inter02\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "        chunk_size = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 5. Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1706614828603
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:40:28.5210512Z",
       "execution_start_time": "2024-01-30T11:40:16.4759845Z",
       "livy_statement_state": "available",
       "parent_msg_id": "672be019-c7ad-41a3-a478-0dac2b9cf53c",
       "queued_time": "2024-01-30T11:40:16.3640359Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 27
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 27, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1706614950947
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:42:30.750752Z",
       "execution_start_time": "2024-01-30T11:42:30.3904412Z",
       "livy_statement_state": "available",
       "parent_msg_id": "6a7a367f-e15a-4982-893f-81a89b429ce5",
       "queued_time": "2024-01-30T11:42:30.2547795Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 28
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 28, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1706614958389
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:42:38.3288571Z",
       "execution_start_time": "2024-01-30T11:42:37.4987601Z",
       "livy_statement_state": "available",
       "parent_msg_id": "327aa731-380f-4ba4-8655-187ae632164d",
       "queued_time": "2024-01-30T11:42:37.3813314Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 29
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 29, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs4/pyenv-aeaa79eb-ee00-4e5f-9c09-bdcef9719555/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(model=\"gpt-35-turbo\",\n",
    "                    deployment_name=\"gpt-35-turbo-inter02\",\n",
    "                    openai_api_key=openai.api_key,\n",
    "                    openai_api_base=openai.api_base,\n",
    "                    openai_api_type=openai.api_type,\n",
    "                    openai_api_version=openai.api_version,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1706614964699
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:42:44.5686325Z",
       "execution_start_time": "2024-01-30T11:42:44.276539Z",
       "livy_statement_state": "available",
       "parent_msg_id": "0d590dec-c73b-4835-80c5-022a46931bb7",
       "queued_time": "2024-01-30T11:42:44.1528918Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 30
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 30, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1706615438920
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:50:38.8905836Z",
       "execution_start_time": "2024-01-30T11:50:36.4203878Z",
       "livy_statement_state": "available",
       "parent_msg_id": "f0ba0983-e399-4e05-8a33-b820a4dbe912",
       "queued_time": "2024-01-30T11:50:36.3061255Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 37
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 37, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is LangChain?\n",
      "\n",
      "\n",
      "Answer: LangChain is a framework for developing applications that utilize large language models (LLMs). It consists of various building blocks called \"chains\" that combine an LLM with a prompt to perform operations on text or other data. LangChain supports different types of chains, such as Simple Sequential Chains and Sequential Chains, which can be used to create sequences of operations. The framework also includes a memory component that stores previous conversations and passes them to the LLM with the next prompt, creating the illusion of memory in chatbot systems. Overall, LangChain enables the development of LLM applications quickly and efficiently.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain?\"\n",
    "answer = qa({\"query\": query})\n",
    "print(query)\n",
    "print(\"\\n\")\n",
    "print(\"Answer:\",answer.get('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1706615481785
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:51:21.7063751Z",
       "execution_start_time": "2024-01-30T11:51:20.1680716Z",
       "livy_statement_state": "available",
       "parent_msg_id": "37b744a9-d511-4b9e-96e4-36ed4eb8bc97",
       "queued_time": "2024-01-30T11:51:19.7222556Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 38
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 38, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are Action Agents?\n",
      "\n",
      "\n",
      "Answer: Action Agents are agents within an agent-based system that operate at a high level by receiving user input, determining the appropriate tool and its input, executing the tool, recording its output, and making decisions on subsequent steps based on the history of tool usage, inputs, and observations. These agents encapsulate the sequence of actions and interactions with the tools, allowing for flexible chains of calls to various tools based on user input.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are Action Agents?\"\n",
    "answer = qa({\"query\": query})\n",
    "print(query)\n",
    "print(\"\\n\")\n",
    "print(\"Answer:\",answer.get('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1706615548482
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:52:28.4544859Z",
       "execution_start_time": "2024-01-30T11:52:23.564348Z",
       "livy_statement_state": "available",
       "parent_msg_id": "733c62fd-3b26-44ec-b606-f57da51f91cd",
       "queued_time": "2024-01-30T11:52:23.4551237Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 39
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 39, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are Action Agents? present you answer in points\n",
      "\n",
      "\n",
      "Answer: Action Agents are agents that operate at a high level and make decisions based on previous actions and observations. Here are some key points about Action Agents:\n",
      "\n",
      "1. Purpose: Action Agents are used in situations where an app needs a flexible chain of calls to Language Model Managers (LLMs) and other tools based on user input.\n",
      "\n",
      "2. Decision-making: Action Agents determine the appropriate tool and its input based on the outputs of all previous actions.\n",
      "\n",
      "3. Execution: Action Agents execute the selected tool, record its output as an observation, and make decisions on subsequent steps based on the history of tool usage, inputs, and observations.\n",
      "\n",
      "4. Cycle: The cycle of receiving user input, determining the tool, executing it, recording the output, and making subsequent decisions repeats until the agent can directly respond to the user.\n",
      "\n",
      "5. Agent Executors: Action Agents are encapsulated in agent executors that manage the sequence of actions and interactions with the tools.\n",
      "\n",
      "6. Interaction with Tools: Action Agents utilize interfaces called tools to facilitate their interactions with the external world.\n",
      "\n",
      "7. Functional Objective: The actions performed by Action Agents are purposefully selected based on the agent's functional objective.\n",
      "\n",
      "8. History-based Decision-making: Action Agents rely on the history of tool usage, inputs, and observations to make informed decisions on subsequent steps.\n",
      "\n",
      "Overall, Action Agents provide a way to dynamically determine the appropriate tools and their inputs based on user input and previous actions, allowing for flexible and adaptive behavior in LLM applications.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are Action Agents? present you answer in points\"\n",
    "answer = qa({\"query\": query})\n",
    "print(query)\n",
    "print(\"\\n\")\n",
    "print(\"Answer:\",answer.get('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1706615592794
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:53:12.7698223Z",
       "execution_start_time": "2024-01-30T11:53:06.4400289Z",
       "livy_statement_state": "available",
       "parent_msg_id": "0918aa10-abb1-4452-b599-9f25d76c63cc",
       "queued_time": "2024-01-30T11:53:06.2969835Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 40
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 40, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are limitations of LLM? present you answer in points\n",
      "\n",
      "\n",
      "Answer: The limitations of Large Language Models (LLMs) are as follows:\n",
      "\n",
      "1. Erroneous Outputs: LLMs can occasionally generate erroneous or illogical outputs, also known as hallucinations. These models may produce sentences or paragraphs that do not make sense or contain incorrect information.\n",
      "\n",
      "2. Lack of Contextual Understanding: LLMs may struggle with understanding the context of a given prompt. They rely heavily on the preceding text to generate coherent responses, which can lead to misunderstandings or irrelevant answers if the context is not properly conveyed.\n",
      "\n",
      "3. Sensitivity to Input Changes: LLMs can be sensitive to slight changes in the input prompt. Modifying a single word or phrase in the prompt can result in significantly different outputs, making it challenging to control or fine-tune the model's responses.\n",
      "\n",
      "4. Lack of Common Sense Reasoning: While LLMs excel at syntactic and grammatical coherence, they often lack common sense reasoning abilities. These models may generate responses that, although grammatically correct, do not align with real-world knowledge or logical reasoning.\n",
      "\n",
      "5. Biases and Inaccuracies: LLMs can inherit biases present in the training data, which may result in biased or inaccurate outputs. These biases can be related to gender, race, or other sensitive topics, leading to potential ethical concerns.\n",
      "\n",
      "6. Overreliance on Training Data: LLMs heavily rely on the training data they are exposed to. If the training data is limited or biased, it can affect the model's performance and its ability to generate accurate and diverse outputs.\n",
      "\n",
      "7. High Computational Requirements: LLMs with a large number of parameters require significant computational resources to train and deploy. This can restrict their accessibility and usability for smaller-scale applications or devices with limited computational capabilities.\n",
      "\n",
      "8. Ethical and Misuse Concerns: The potential misuse of LLMs for generating misleading information, spreading disinformation, or creating deepfakes raises ethical concerns. Ensuring responsible use and addressing the consequences of such misuse is an ongoing challenge.\n",
      "\n",
      "It's important to note that advancements in research and techniques are continuously addressing these limitations, but they still remain relevant considerations when working with LLMs.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are limitations of LLM? present you answer in points\"\n",
    "answer = qa({\"query\": query})\n",
    "print(query)\n",
    "print(\"\\n\")\n",
    "print(\"Answer:\",answer.get('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "gather": {
     "logged": 1706615763450
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:56:03.4226483Z",
       "execution_start_time": "2024-01-30T11:56:01.934056Z",
       "livy_statement_state": "available",
       "parent_msg_id": "88a64337-e030-4dec-8b59-f9c65f4cfbe6",
       "queued_time": "2024-01-30T11:56:01.7971239Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 41
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 41, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the use cases mentioned in the document\n",
      "\n",
      "\n",
      "Answer: The document mentions several use cases for the LangChain framework. Some of the mentioned use cases include:\n",
      "- Autonomous agents\n",
      "- Chatbots\n",
      "- Code understanding agents\n",
      "- Extraction\n",
      "- Question answering over documents\n",
      "- Summarization\n",
      "- Analyzing structured data\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the use cases mentioned in the document\"\n",
    "answer = qa({\"query\": query})\n",
    "print(query)\n",
    "print(\"\\n\")\n",
    "print(\"Answer:\",answer.get('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gather": {
     "logged": 1706615793990
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-01-30T11:56:33.9652366Z",
       "execution_start_time": "2024-01-30T11:56:32.4421698Z",
       "livy_statement_state": "available",
       "parent_msg_id": "1149a6fc-bea8-4e1e-87c2-43cc4d496e5c",
       "queued_time": "2024-01-30T11:56:32.317906Z",
       "session_id": "525",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "5e7fc659-e1d1-4302-b80f-a5f1f8c968f2",
       "state": "finished",
       "statement_id": 42
      },
      "text/plain": [
       "StatementMeta(5e7fc659-e1d1-4302-b80f-a5f1f8c968f2, 525, 42, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is best US President?\n",
      "\n",
      "\n",
      "Answer: Based on the provided context, there is no information or discussion about the best US President. Therefore, I don't have the information to answer your question.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is best US President?\"\n",
    "answer = qa({\"query\": query})\n",
    "print(query)\n",
    "print(\"\\n\")\n",
    "print(\"Answer:\",answer.get('result'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Rajat.Chaudhari"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
